# -*- mode: ruby -*-
# vi: set ft=ruby :
# :box_version => "20180831.0.0",

APISERVER_IP="192.168.205.11"
K8S_USER="vagrant"

servers = [
    {
        :name => "k8s-master",
        :type => "master",
        :box => "ubuntu/xenial64",
        :box_version => "20200204.0.0",        
        :eth1 => "192.168.205.11",
        :mem => "4096",
        :cpu => "2"
    },
    {
        :name => "k8s-node-1",
        :type => "node",
        :box => "ubuntu/xenial64",
        :box_version => "20200204.0.0",
        :eth1 => "192.168.205.12",
        :mem => "4096",
        :cpu => "2"
    },
    {
        :name => "k8s-node-2",
        :type => "node",
        :box => "ubuntu/xenial64",
        :box_version => "20200204.0.0",
        :eth1 => "192.168.205.13",
        :mem => "4096",
        :cpu => "2"
    },
    {
        :name => "k8s-admin",
        :type => "admin",
        :box => "ubuntu/xenial64",
        :box_version => "20200204.0.0",        
        :eth1 => "192.168.205.10",
        :mem => "1024",
        :cpu => "1"
    }
]

# This script to install k8s using kubeadm will get executed after a box is provisioned
$configureBox = <<-SCRIPT

    # bridged traffic to iptables is enabled for kube-router. - 2020.02.05
    cat >> /etc/ufw/sysctl.conf <<EOF
    net/bridge/bridge-nf-call-ip6tables = 1
    net/bridge/bridge-nf-call-iptables = 1
    net/bridge/bridge-nf-call-arptables = 1
EOF

    # add nameserver
    cat >> /etc/resolv.conf <<EOF
    nameserver 8.8.4.4
    nameserver 8.8.8.8
EOF


    # 2020.02.05
    export DEBIAN_FRONTEND=noninteractive
    # sudo vbox-uninstall-guest-additions

    # install docker v19.03
    # reason for not using docker provision is that it always installs latest version of the docker, but kubeadm requires 19.03 or older
    apt-get update


    apt-get install -y apt-transport-https ca-certificates curl software-properties-common ebtables ethtool sshpass jq

    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
    add-apt-repository "deb https://download.docker.com/linux/$(. /etc/os-release; echo "$ID") $(lsb_release -cs) stable"
    apt-get update && apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 19.03 | head -1 | awk '{print $3}')

    # run docker commands as vagrant user (sudo not required)
    usermod -aG docker vagrant

    # install kubeadm
    # apt-get install -y apt-transport-https curl
    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
    cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
    deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF

    apt-get update
    apt-get install -y kubelet kubeadm kubectl
    apt-mark hold kubelet kubeadm kubectl

    # kubelet requires swap off
    swapoff -a

    # keep swap off after reboot
    sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab


    # ip of this box
    IP_ADDR=`ifconfig enp0s8 | grep Mask | awk '{print $2}'| cut -f2 -d:`
    # set node-ip
    sudo sed -i "/^[^#]*KUBELET_EXTRA_ARGS=/c\KUBELET_EXTRA_ARGS=--node-ip=$IP_ADDR" /etc/default/kubelet
    sudo systemctl restart kubelet

    # required for setting up password less ssh between k8s VMs
    sudo sed -i "/^[^#]*PasswordAuthentication[[:space:]]no/c\PasswordAuthentication yes" /etc/ssh/sshd_config
    # sudo service sshd restart
    sudo systemctl restart sshd

    cat <<EoF >> /home/vagrant/.profile
    set -o vi

    PATH=${HOME}/bin:$PATH; export PATH
    alias ls="ls -xF"
    alias rm="rm -i"

    alias kc="kubectl -n kube-system"
EoF

    sudo --user=vagrant mkdir /home/vagrant/bin
    chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/bin

SCRIPT

$configureMaster = <<-SCRIPT
    echo "This is master"
    # ip of this box
    IP_ADDR=`ifconfig enp0s8 | grep Mask | awk '{print $2}'| cut -f2 -d:`

    # install k8s master ( Start cluster )
    HOST_NAME=$(hostname -s)
    kubeadm init --apiserver-advertise-address=$IP_ADDR --apiserver-cert-extra-sans=$IP_ADDR  --node-name $HOST_NAME --pod-network-cidr=172.16.0.0/16

    # copying credentials to regular user - vagrant
    sudo --user=vagrant mkdir -p /home/vagrant/.kube
    cp -i /etc/kubernetes/admin.conf /home/vagrant/.kube/config
    chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/.kube/config


    # Deploy Pod Network
    export KUBECONFIG=/etc/kubernetes/admin.conf
    # Configure flannel
    # curl -o kube-flannel.yml https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
    # sed -i.bak 's|"/opt/bin/flanneld",|"/opt/bin/flanneld", "--iface=enp0s8",|' kube-flannel.yml
    # kubectl create -f kube-flannel.yml

    # install Calico pod network addon
    kubectl apply -f https://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
    # Installing with the Kubernetes API datastore—50 nodes or less
    curl https://docs.projectcalico.org/v3.9/manifests/calico.yaml -O
    kubectl apply -f calico.yaml
    # Installing with the Kubernetes API datastore—more than 50 nodes
    # curl https://docs.projectcalico.org/v3.9/manifests/calico-typha.yaml -o calico.yaml
    # kubectl apply -f calico-typha.yaml

    # Installing with the etcd datastore
    # curl https://docs.projectcalico.org/v3.9/manifests/calico-etcd.yaml -o calico-etcd.yaml
    # kubectl apply -f calico-etcd.yaml

    # Taint the master node for allowing deployment
    # kubectl taint nodes --all node-role.kubernetes.io/master-

    kubeadm token create --print-join-command >> /etc/kubeadm_join_cmd.sh
    chmod +x /etc/kubeadm_join_cmd.sh

    # # required for setting up password less ssh between guest VMs
    # sudo sed -i "/^[^#]*PasswordAuthentication[[:space:]]no/c\PasswordAuthentication yes" /etc/ssh/sshd_config
    # # sudo service sshd restart
    # sudo systemctl restart sshd


    export DOWNLOAD_URL=$(curl -Ls "https://api.github.com/repos/kubernetes-sigs/metrics-server/releases/latest" | jq -r .tarball_url)
    export DOWNLOAD_VERSION=$(grep -o '[^/v]*$' <<< $DOWNLOAD_URL)
    curl -Ls $DOWNLOAD_URL -o metrics-server-$DOWNLOAD_VERSION.tar.gz
    mkdir metrics-server-$DOWNLOAD_VERSION
    tar -xzf metrics-server-$DOWNLOAD_VERSION.tar.gz --directory metrics-server-$DOWNLOAD_VERSION --strip-components 1
    kubectl apply -f metrics-server-$DOWNLOAD_VERSION/deploy/1.8+/
    kubectl get deployment metrics-server -n kube-system
    kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml
    # kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 10443:443

    cat <<EOF >admin-user-sa.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:    
      name: admin-user
      namespace: kubernetes-dashboard
EOF
    kubectl apply -f admin-user-sa.yaml

    cat <<EOF >cluster-admin-crb.yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: kubernetes-dashboard
      namespace: kubernetes-dashboard
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
      - kind: ServiceAccount
        name: kubernetes-dashboard
        namespace: kubernetes-dashboard
EOF
    kubectl apply -f cluster-admin-crb.yaml

    cat <<EOF >admin-user-crb.yaml
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: admin-user
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
    - kind: ServiceAccount
    name: admin-user
    namespace: kubernetes-dashboard
EOF

    kubectl apply -f admin-user-crb.yaml

    cat <<EOF >get-token.sh
    #!/bin/bash
    # echo "# Bearer Token"
    kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
EOF
    sudo chmod +x get-token.sh

SCRIPT



$configureNode = <<-SCRIPT
    echo "This is worker"



    # Add Worker Node to the Cluster
    sshpass -p "vagrant" scp -o StrictHostKeyChecking=no vagrant@192.168.205.11:/etc/kubeadm_join_cmd.sh .
    sh ./kubeadm_join_cmd.sh

    # Configure kubectl
    # copying credentials to regular user - vagrant
    sudo --user=vagrant mkdir -p /home/vagrant/.kube
    sshpass -p "vagrant" scp -o StrictHostKeyChecking=no vagrant@192.168.205.11:/home/vagrant/.kube/config /home/vagrant/.kube/config
    chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/.kube/config
    export KUBECONFIG=/home/vagrant/.kube/config
    
    cat <<EOF >/home/vagrant/.profile
    export KUBECONFIG=/home/vagrant/.kube/config
EOF

SCRIPT


$configureAdmin = <<-SCRIPT
    echo "This is admin"


    # copying credentials to regular user - vagrant
    sudo --user=vagrant mkdir -p /home/vagrant/.kube
    sshpass -p "vagrant" scp -o StrictHostKeyChecking=no vagrant@192.168.205.11:/home/vagrant/.kube/config /home/vagrant/.kube/config
    chown $(id -u vagrant):$(id -g vagrant) /home/vagrant/.kube/config
    export KUBECONFIG=/home/vagrant/.kube/config
    echo "export KUBECONFIG=/home/vagrant/.kube/config" /home/vagrant/.profile

    # Helm
    mkdir -p ~/helm
    cd ~/helm
    curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > ~/helm/get_helm.sh
    chmod +x ~/helm/get_helm.sh
    ~/helm/get_helm.sh

    cat <<EoF > ~/helm/tiller-sa.yaml
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: tiller
      namespace: kube-system
EoF
    kubectl apply -f ~/helm/tiller-sa.yaml


    cat <<EoF > ~/helm/tiller-crb.yaml
    apiVersion: rbac.authorization.k8s.io/v1beta1
    kind: ClusterRoleBinding
    metadata:
      name: tiller
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: cluster-admin
    subjects:
      - kind: ServiceAccount
        name: tiller
        namespace: kube-system
EoF
    kubectl apply -f ~/helm/tiller-crb.yaml
    helm init --service-account tiller

    # ADD THE BITNAMI REPOSITORY
    helm repo add bitnami https://charts.bitnami.com/bitnami
    helm repo add istio.io https://storage.googleapis.com/istio-release/releases/1.1.7/charts/
    helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/
    helm repos add kubernetes-charts https://kubernetes-charts.storage.googleapis.com/

    cp -rf /root/.helm /home/vagrant/.helm
    chown $(id -u vagrant):$(id -g vagrant) -R /home/vagrant/.helm
    cd ~

    # DOWNLOAD AND INSTALL ISTIO CLI
    curl -L https://istio.io/downloadIstio | sh -
    cd istio-*
    sudo cp -v bin/istioctl /usr/local/bin


    # INSTALL ISTIO
    

    # install Istio using the Helm
    # kubectl create namespace istio-system
    # helm template install/kubernetes/helm/istio --name istio --namespace istio-system --set gateways.istio-ingressgateway.type=NodePort | kubectl apply -f -

    # Install Istio using the default profile
    istioctl manifest apply --set profile=demo    
    # Display the list of available profiles
    istioctl profile list
    kubectl label namespace default istio-injection=enabled


    # Install from external charts
    # istioctl manifest apply --set installPackagePath=~/istio-releases/istio-1.4.3/install/kubernetes/operator/charts

    # Deploy Sample Apps
    # kubectl apply -f <(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)
    # # Next we'll define the virtual service and ingress gateway
    # kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
    # kubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' -n istio-system ; echo
    # kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml
    # kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml
    # kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml
    # kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml
    # kubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml
    # kubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml
    # kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml

    # MONITORING & VISUTALIZE
    # curl -LO https://eksworkshop.com/servicemesh/deploy.files/istio-telemetry.yaml
    # kubectl apply -f istio-telemetry.yaml
    # kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 8080:3000 &


  
SCRIPT


Vagrant.configure("2") do |config|

    servers.each do |opts|
        config.vm.define opts[:name] do |config|

            config.vm.box = opts[:box]
            config.vm.box_version = opts[:box_version]
            config.vm.hostname = opts[:name]
            config.vm.network :private_network, ip: opts[:eth1]

            config.vm.provider "virtualbox" do |v|

                v.name = opts[:name]
            	v.customize ["modifyvm", :id, "--groups", "/k8s"]
                v.customize ["modifyvm", :id, "--memory", opts[:mem]]
                v.customize ["modifyvm", :id, "--cpus", opts[:cpu]]                

            end

            # we cannot use this because we can't install the docker version we want - https://github.com/hashicorp/vagrant/issues/4871
            # config.vm.provision "docker"

            config.vm.provision "shell", inline: $configureBox

            if opts[:type] == "master"
                 config.vm.provision "shell", inline: $configureMaster
            end
            if opts[:type] == "node"                
                config.vm.provision "shell", inline: $configureNode
            end
            if opts[:type] == "admin"                
                config.vm.provision "shell", inline: $configureAdmin
            end

        end

    end

end 